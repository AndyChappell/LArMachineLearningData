{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relevant scripts from LArMachineLearningData\n",
    "# Nice the process so it can run with lots of cores on low priority\n",
    "import os\n",
    "os.nice(20)\n",
    "\n",
    "# Add path for LArMachineLearningData\n",
    "import sys\n",
    "pandoraMVADir = os.environ['MY_TEST_AREA'] + '/LArMachineLearningData/'\n",
    "sys.path.append(pandoraMVADir + 'scripts')\n",
    "\n",
    "from PandoraBDT import *\n",
    "\n",
    "# Import relevant SKLearn stuff\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set global params\n",
    "testTrainFraction = 0.5\n",
    "nCores = -1\n",
    "use_hierarchy_vars = False # This should be False when training BDTs for use before a neutrino hierarchy has been constructed, and True for those after\n",
    "use_charge_info = True # This should be alternately set to True and False to train two different BDTs for both pre and post hierarchy cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bdt_parameters(use_hierarchy_vars=False, use_charge_info=True):\n",
    "    if not use_hierarchy_vars:\n",
    "        bdt_name = 'PreHierarchy' if use_charge_info else 'PreHierarchy_NoChargeInfo'\n",
    "        filename = 'PfoCharBdtPreHierarchyTrain.txt' if use_charge_info else 'PfoCharBdtPreHierarchyTrainnoChargeInfo.txt'\n",
    "        bdt_features = ['Length', 'Straight Line Diff Mean', 'Max Fit Gap Length', 'Sliding Linear Fit RMS',\n",
    "            'Vertex Distance', 'PCA Secondary-Primary EigenValue Ratio', 'PCA Tertiary-Primary EigenValue Ratio',\n",
    "            'Opening Angle Diff', 'Fractional Spread', 'End Fraction']\n",
    "        \n",
    "        return filename, bdt_name, bdt_features\n",
    "    else:\n",
    "        bdt_name = 'PostHierarchy' if use_charge_info else 'PostHierarchy_NoChargeInfo'\n",
    "        filename = 'PfoCharBdtPostHierarchyTrain.txt' if use_charge_info else 'PfoCharBdtPostHierarchyTrainnoChargeInfo.txt'\n",
    "        bdt_features = ['Length', 'Straight Line Diff Mean', 'Max Fit Gap Length',  'Sliding Linear Fit RMS',\n",
    "            'Vertex Distance', 'PCA Secondary-Primary EigenValue Ratio', 'PCA Tertiary-Primary EigenValue Ratio',\n",
    "            'Hierarchy N Daughters', 'Hierarchy N Daughters Hits 3D', 'Hierarchy Daughter-Parent Hit Ratio',\n",
    "            'Opening Angle Diff', 'Fractional Spread', 'End Fraction']\n",
    "        \n",
    "        return filename, bdt_name, bdt_features\n",
    "\n",
    "input_filename, BDTName, featureNames = get_bdt_parameters(use_hierarchy_vars, use_charge_info)\n",
    "trainingFile = pandoraMVADir + input_filename\n",
    "\n",
    "# Set background and signal label names\n",
    "params = {\n",
    "    'labelNames': ['True Shower','True Track'],\n",
    "    'signalDefs': [0, 1],\n",
    "    'signalCols': ['r', 'b'],\n",
    "    'nBins': 100,\n",
    "    'PlotStep': 1.0,\n",
    "    'OptimalBinCut': 50,\n",
    "    'OptimalScoreCut': 0.5,\n",
    "    'nTrees': 100,\n",
    "    'TreeDepth': 3,\n",
    "    'logY': False\n",
    "}\n",
    "\n",
    "# Create the base BDT to vary the params from and compare to\n",
    "baseBDT = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=params['TreeDepth']),algorithm='SAMME', \n",
    "                         random_state=42, n_estimators=params['nTrees'])\n",
    "\n",
    "# Split the data into many subsets to grid search over (Set seed for reproducibility)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data, nFeatures, nExamples = LoadData(trainingFile, ',')\n",
    "featuresOrg, labelsOrg = SplitTrainingSet(data, nFeatures)\n",
    "features, labels = Randomize(featuresOrg, labelsOrg, True)\n",
    "\n",
    "# Split into train and test samples\n",
    "xTrain, yTrain, xTest, yTest = Sample(features, labels, testTrainFraction)\n",
    "\n",
    "# Split into signal and background based on the true labels\n",
    "signalFeatures = features[labels==1]\n",
    "backgroundFeatures = features[labels==0]\n",
    "\n",
    "# Check the features array is the same size as the feature names array\n",
    "print (len(featureNames))\n",
    "print (np.shape(features))\n",
    "print('Total: '+str(len(features))+', signal: '+\n",
    "      str(len(signalFeatures))+' and background: '+\n",
    "      str(len(backgroundFeatures)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Pandas dataframe\n",
    "# First crete a dictionary\n",
    "allDict = {featureNames[i]: features[:, i] for i in range(nFeatures)}\n",
    "allDict.update({'Labels': labels})\n",
    "\n",
    "# Create the Pandas dataframe, create seperate df for signal/background\n",
    "df = pd.DataFrame(data=allDict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make plots drawing the variables for signal/background\n",
    "DrawVariablesDF(df, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make correlation matricies\n",
    "dfSig = df[df['Labels']==params['signalDefs'][0]].drop('Labels', axis=1)\n",
    "dfBck = df[df['Labels']==params['signalDefs'][1]].drop('Labels', axis=1)\n",
    "\n",
    "CorrelationDF(dfSig, params['labelNames'][0] + ' Correlation Matrix')\n",
    "CorrelationDF(dfBck, params['labelNames'][1] + ' Correlation Matrix')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# If we want to make a plot comparing two variables;\n",
    "xMetric = 'Vertex Distance'\n",
    "yMetric = 'PCA Secondary-Primary EigenValue Ratio'\n",
    "\n",
    "sns.jointplot(data=df, x=xMetric, y=yMetric, hue='Labels',\n",
    "              xlim=(np.quantile(df[xMetric], 0.02), np.quantile(df[xMetric], 0.98)), \n",
    "              ylim=(np.quantile(df[yMetric], 0.02), np.quantile(df[yMetric], 0.98)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# For plotting all combos, not very useful when we have too many variables\n",
    "sns.pairplot(df, hue='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of grid search\n",
    "depthRange = 3\n",
    "treeRange = 3\n",
    "\n",
    "# Set up ranges for grid search\n",
    "depthArray = np.linspace(1, depthRange, depthRange, dtype=int)\n",
    "treeArray = np.logspace(0, treeRange-1, treeRange, dtype=int)\n",
    "#treeArray = np.linspace(100, 100*treeRange, treeRange, dtype=int)\n",
    "\n",
    "# Print arrays for debugging\n",
    "print (\"Depth Array:\", depthArray)\n",
    "print (\"Tree Array: \", treeArray)\n",
    "\n",
    "# Construct a dictionary to loop over\n",
    "paramGrid = dict(estimator__max_depth=depthArray, n_estimators=treeArray)\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(baseBDT, param_grid=paramGrid, cv=cv, n_jobs=nCores, \n",
    "                    verbose=9, refit=True, return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search\n",
    "grid.fit(xTrain, yTrain)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"% \n",
    "      (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# Put the output of the grid in a conveneant df\n",
    "gridResults = pd.DataFrame(grid.cv_results_)\n",
    "gridResults.rename(columns={\"param_estimator__max_depth\": \"MaxDepth\"}, inplace=True)\n",
    "gridResults.rename(columns={\"param_n_estimators\": \"NTrees\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testScores = gridResults.pivot(index = \"MaxDepth\", columns=\"NTrees\", values=\"mean_test_score\")\n",
    "testStd = gridResults.pivot(index = \"MaxDepth\", columns=\"NTrees\", values=\"std_test_score\")\n",
    "trainScores = gridResults.pivot(index = \"MaxDepth\", columns=\"NTrees\", values=\"mean_train_score\")\n",
    "\n",
    "trainTestDiff = trainScores - testScores\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(testScores, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Test')\n",
    "plt.gca().invert_yaxis()\n",
    "save_figure(plt, 'TestScores')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(testStd, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Std Test Score')\n",
    "plt.gca().invert_yaxis()\n",
    "save_figure(plt, 'TrainStds')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(trainScores, cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Train')\n",
    "plt.gca().invert_yaxis()\n",
    "save_figure(plt, 'TrainScores')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4), constrained_layout=True)\n",
    "sns.heatmap(abs(trainTestDiff), cmap='bwr', linewidths=0, annot=True)\n",
    "plt.title('Validation accuracy: Train Test Diff')\n",
    "plt.gca().invert_yaxis()\n",
    "save_figure(plt, 'TrainTestDiff')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter selection\n",
    "Choose the number of trees and the tree depth according to the peak accuracy and lowest deviation of the test performance with respect to the training performance from the above runs and recreate the base BDT using this selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['nTrees'] = 100\n",
    "params['TreeDepth'] = 2\n",
    "\n",
    "baseBDT = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=params['TreeDepth']),algorithm='SAMME', \n",
    "                         random_state=42, n_estimators=params['nTrees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference BDT with controlled hyperparams\n",
    "baseBDT.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more detailed performance info\n",
    "bdtPredicted = baseBDT.predict(xTest)\n",
    "gridPredicted = grid.predict(xTest)\n",
    "\n",
    "print (\"Background (0): \", params['labelNames'][0])\n",
    "print (\"Signal (1): \", params['labelNames'][1])\n",
    "print (\"BDT:\\n\", metrics.classification_report(yTest, bdtPredicted))\n",
    "print (\"Grid:\\n\", metrics.classification_report(yTest, gridPredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search performance over training sample size\n",
    "train_sizes_array = np.linspace(0.0,1, 20)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(baseBDT, features,\n",
    "    labels, train_sizes=train_sizes_array[1:], n_jobs=nCores, verbose=9, cv=cv)\n",
    "\n",
    "mean_train_scores = np.mean(train_scores, axis=1)\n",
    "mean_test_scores = np.mean(test_scores, axis=1)\n",
    "\n",
    "std_train_scores = np.std(train_scores, axis=1)\n",
    "std_test_scores = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progression\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Training Progression\")\n",
    "plt.xlabel(\"Number of Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.plot(train_sizes, mean_train_scores, label='Train Score', color='b')\n",
    "plt.fill_between(train_sizes, mean_train_scores - std_train_scores,\n",
    "                         mean_train_scores + std_train_scores, alpha=0.1,\n",
    "                         color=\"b\")\n",
    "\n",
    "plt.plot(train_sizes, mean_test_scores, label='Test Score', color='r')\n",
    "plt.fill_between(train_sizes, mean_test_scores - std_test_scores,\n",
    "                         mean_test_scores + std_test_scores, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "#plt.plot(train_sizes, std_test_scores, label='Test Score Std.', color='k')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "save_figure(plt, \"hyper_param_dataset_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set size selection\n",
    "If needed, adjust the size of the data set to ensure adequate fitting, while avoiding over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = xTrain.shape[0] # xTrain.shape[0] implies full dataset, otherwise use a value half the size of the reported optimal number of training examples\n",
    "xTrain = xTrain[:subset_size]\n",
    "yTrain = yTrain[:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search over a metric\n",
    "learningRateArray = np.linspace(0.2,1.8,9)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    baseBDT, features, labels, param_name='learning_rate',\n",
    "    param_range=learningRateArray, n_jobs=nCores, verbose=9, cv=cv)\n",
    "\n",
    "mean_train_scores = np.mean(train_scores, axis=1)\n",
    "mean_test_scores = np.mean(test_scores, axis=1)\n",
    "\n",
    "std_train_scores = np.std(train_scores, axis=1)\n",
    "std_test_scores = np.std(test_scores, axis=1)\n",
    "\n",
    "print (\"Means: \"+str(mean_test_scores)+\" and std. \"\n",
    "       +str(std_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot grid search\n",
    "plt.plot(learningRateArray, mean_train_scores, label='Train Score', color='b')\n",
    "plt.fill_between(learningRateArray, mean_train_scores - std_train_scores,\n",
    "                         mean_train_scores + std_train_scores, alpha=0.1,\n",
    "                         color=\"b\")\n",
    "plt.plot(learningRateArray, mean_test_scores, label='Test Score', color='r')\n",
    "plt.fill_between(learningRateArray, mean_test_scores - std_test_scores,\n",
    "                         mean_test_scores + std_test_scores, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "plt.grid()\n",
    "#plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "save_figure(plt, \"hyper_param_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "Update the learning rate based on the previous evaluation. Look for high accuracy without evidence of over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1.6 # default is 1\n",
    "baseBDT = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=params['TreeDepth']),algorithm='SAMME', \n",
    "                         random_state=42, n_estimators=params['nTrees'], learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final BDT fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseBDT.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more detailed performance info\n",
    "bdtPredicted = baseBDT.predict(xTest)\n",
    "\n",
    "print (\"Background (0): \", params['labelNames'][0])\n",
    "print (\"Signal (1): \", params['labelNames'][1])\n",
    "print (\"BDT:\\n\", metrics.classification_report(yTest, bdtPredicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(tpr, fpr, label='ROC')\n",
    "    plt.plot([0,1], [1,0], linestyle = '--')\n",
    "    plt.xlabel('True Negative Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    save_figure(plt, \"roc_curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = baseBDT.predict_proba(xTest)\n",
    "probs = probs[:, 1]\n",
    "fpr, tpr, _ = metrics.roc_curve(yTest, probs)\n",
    "tnr = 1 - fpr\n",
    "plot_roc_curve(tnr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = baseBDT.predict(xTest)\n",
    "\n",
    "# Plot Confusion Matricies\n",
    "display = metrics.ConfusionMatrixDisplay.from_estimator(baseBDT, xTest, yTest, normalize='true', display_labels=params['labelNames'], colorbar=True, cmap='Blues')\n",
    "display.im_.set_clim(0, 1)\n",
    "save_figure(plt, \"confusion_matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance of features\n",
    "importanceDF = pd.DataFrame({'Features': featureNames, 'Importance Score':baseBDT.feature_importances_})\n",
    "print (importanceDF.sort_values(by=['Importance Score']))\n",
    "ax = importanceDF.sort_values(by=['Importance Score'])\\\n",
    "    .plot(kind='barh', x='Features', y='Importance Score')\n",
    "save_figure(plt, \"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all tunable params\n",
    "baseBDT.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PandoraBDT\n",
    "from importlib import reload\n",
    "\n",
    "reload (PandoraBDT)\n",
    "from PandoraBDT import *\n",
    "\n",
    "print (np.shape(xTest))\n",
    "print (np.shape(yTest))\n",
    "print (np.shape(xTrain))\n",
    "print (np.shape(yTrain))\n",
    "\n",
    "\n",
    "PlotBdtKSScores(baseBDT, xTest, yTest, xTrain, yTrain, 'Vertex Region', params)\n",
    "save_figure(plt, \"ks_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteXmlFile(BDTName+\".xml\", baseBDT, BDTName)\n",
    "SerializeToPkl(BDTName+\".pkl\", baseBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
