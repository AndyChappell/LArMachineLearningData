{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12b7a8-fc0c-437b-9a10-86010f6ebd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset suitable for segmentation tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, mask_dir, filenames, transform=None, device=torch.device('cuda:0')):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                image_dir: The directory containing the images\n",
    "                mask_dir: The directory containing the masks\n",
    "                filenames: The filanems for the images associate with this dataset\n",
    "                transform: Optional transform to be applied on a sample (default: None).\n",
    "                device: The device on which tensors should be created (default: 'cuda:0')\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.filenames = filenames\n",
    "        self.shift = 0.\n",
    "        self.norm = 255.\n",
    "        self.normalise = False\n",
    "        self.device = device\n",
    "\n",
    "    def set_image_stats(self, shift, norm):\n",
    "        \"\"\"Set the image normalisation parameters.\n",
    "\n",
    "            Applied as norm_val = (val-shift) / norm\n",
    "\n",
    "            Args:\n",
    "                shift: The shift parameter (e.g. mean)\n",
    "                norm: The normalisation parameter (e.g. standard deviation)\n",
    "        \"\"\"\n",
    "        self.shift = shift\n",
    "        self.norm = norm\n",
    "\n",
    "    def set_normalisation(self, norm=True):\n",
    "        \"\"\"Sets whether or not the image should be normalised.\n",
    "\n",
    "            Args:\n",
    "                norm: True if the image should be normalised, False otherwise (default: True)\n",
    "        \"\"\"\n",
    "        self.normalise = norm\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retrieve the number of samples in the dataset.\n",
    "\n",
    "            Returns:\n",
    "                The number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a sample from the dataset.\n",
    "\n",
    "            Args:\n",
    "                idx: The index of the sample to be retrieved\n",
    "\n",
    "            Returns:\n",
    "                The sample requested\n",
    "        \"\"\"\n",
    "        img_name = os.path.join(self.image_dir, self.filenames[idx])\n",
    "        image = np.asarray(self.open_image(img_name)).astype(np.float32)\n",
    "\n",
    "        mask_name = os.path.join(self.mask_dir, self.filenames[idx])\n",
    "        # When using categorical cross entropy, need an un-normalised long\n",
    "        mask = np.asarray(self.open_image(mask_name)).astype(np.int_)\n",
    "\n",
    "        image = torch.as_tensor(np.expand_dims(image, axis=0), device=self.device, dtype=torch.float)\n",
    "        if self.normalise:\n",
    "            image -= self.shift\n",
    "            image /= self.norm\n",
    "        mask = torch.as_tensor(mask, device=self.device, dtype=torch.long)\n",
    "\n",
    "        return (image, mask)\n",
    "\n",
    "    def open_image(self, path):\n",
    "        \"\"\"Retrieve an image.\n",
    "\n",
    "            Args:\n",
    "                path: The path of the image\n",
    "\n",
    "            Returns:\n",
    "                The image\n",
    "        \"\"\"\n",
    "        from PIL import Image\n",
    "        img = Image.open(path)\n",
    "        if img.mode  != 'L':\n",
    "            img = img.convert('L')\n",
    "        return img\n",
    "\n",
    "class SegmentationBunch():\n",
    "    \"\"\"Associates batches of training, validation and testing datasets suitable\n",
    "        for segmentation tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_dir, mask_dir, batch_size, valid_pct=0.25,\n",
    "                 test_pct=0.0, transform=None, device=torch.device('cuda:0')):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                root_dir: The top-level directory containing the images\n",
    "                image_dir: The relative directory containing the images\n",
    "                mask_dir: The relative directory containing the masks\n",
    "                batch_size: The batch size\n",
    "                valid_pct: The fraction of images to be used for validation (default: 0.1)\n",
    "                test_pct: The fraction of images to be used for testing (default: 0.0)\n",
    "                transform: Any transforms to be applied to the images (default: None)\n",
    "                device: The device on which tensors should be created (default: 'cuda:0')\n",
    "        \"\"\"\n",
    "        assert((valid_pct + test_pct) < 1.)\n",
    "        image_dir = os.path.join(root_dir, image_dir)\n",
    "        mask_dir = os.path.join(root_dir, mask_dir)\n",
    "        transform = transform\n",
    "        image_filenames = np.array(next(os.walk(image_dir))[2])\n",
    "\n",
    "        n_files = len(image_filenames)\n",
    "        valid_size = int(n_files * valid_pct)\n",
    "        train_size = n_files - valid_size\n",
    "        sample = np.random.permutation(n_files)\n",
    "        train_sample = sample[valid_size:] if not train_size else \\\n",
    "            sample[valid_size:valid_size + train_size]\n",
    "        valid_sample = sample[:valid_size]\n",
    "        \n",
    "        train_filenames = image_filenames[train_sample]\n",
    "        valid_filenames = image_filenames[valid_sample]\n",
    "        print(valid_filenames[0:10])\n",
    "\n",
    "        train_ds = SegmentationDataset(image_dir, mask_dir, train_filenames, transform, device)\n",
    "        train_ds.set_image_stats(*self.image_stats())\n",
    "        train_ds.set_normalisation(True)\n",
    "        self.train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=0)\n",
    "\n",
    "        valid_ds = SegmentationDataset(image_dir, mask_dir, valid_filenames, transform, device)\n",
    "        valid_ds.set_image_stats(*self.image_stats())\n",
    "        valid_ds.set_normalisation(True)\n",
    "        self.valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=0)\n",
    "\n",
    "        self.test_dl = None\n",
    "\n",
    "    def image_stats(self):\n",
    "        \"\"\"Retrieve the normalisation statistics.\n",
    "\n",
    "            Returns:\n",
    "                The normalisation statistics as the tuple (shift, norm)\n",
    "        \"\"\"\n",
    "        return 0., 255.\n",
    "\n",
    "    def count_classes(self, num_classes):\n",
    "        \"\"\"Count the number of instances of each class in the training set\n",
    "\n",
    "            Args:\n",
    "                num_classes: The number of classes in the training set\n",
    "\n",
    "            Returns:\n",
    "                A list of the number of instances of each class\n",
    "        \"\"\"\n",
    "        count = np.zeros(num_classes)\n",
    "        for batch in self.train_dl:\n",
    "            _, truth = batch\n",
    "            unique, counts = torch.unique(truth, return_counts=True)\n",
    "            unique = [ u.item() for u in unique ]\n",
    "            counts = [ c.item() for c in counts ]\n",
    "            this_dict = dict(zip(unique, counts))\n",
    "            for key in this_dict:\n",
    "                count[key] += this_dict[key]\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba8d1a-3a3e-400f-9015-eabe7a7bf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def maxpool():\n",
    "    \"\"\"Return a max pooling layer.\n",
    "\n",
    "        The maxpooling layer has a kernel size of 2, a stride of 2 and no padding.\n",
    "\n",
    "        Returns:\n",
    "            The max pooling layer\n",
    "    \"\"\"\n",
    "    return nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n",
    "\n",
    "\n",
    "def dropout(prob):\n",
    "    \"\"\"Return a dropout layer.\n",
    "\n",
    "        Args:\n",
    "            prob: The probability that drop out will be applied.\n",
    "\n",
    "        Returns:\n",
    "            The dropout layer\n",
    "    \"\"\"\n",
    "    return nn.Dropout(prob)\n",
    "\n",
    "\n",
    "def reinit_layer(seq_block, leak = 0.0, use_kaiming_normal=True):\n",
    "    \"\"\"Reinitialises convolutional layer weights.\n",
    "\n",
    "        The default Kaiming initialisation in PyTorch is not optimal, this method\n",
    "        reinitialises the layers using better parameters\n",
    "\n",
    "        Args:\n",
    "            seq_block: The layer to be reinitialised.\n",
    "            leak: The leakiness of ReLU (default: 0.0)\n",
    "            use_kaiming_normal: Use Kaiming normal if True, Kaiming uniform otherwise (default: True)\n",
    "    \"\"\"\n",
    "    for layer in seq_block:\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.ConvTranspose2d):\n",
    "            if use_kaiming_normal:\n",
    "                nn.init.kaiming_normal_(layer.weight, a = leak)\n",
    "            else:\n",
    "                nn.init.kaiming_uniform_(layer.weight, a = leak)\n",
    "                layer.bias.data.zero_()\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"A convolution block\n",
    "    \"\"\"\n",
    "\n",
    "    # Sigmoid activation suitable for binary cross-entropy\n",
    "    def __init__(self, c_in, c_out, k_size = 3, k_pad = 1):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                c_in: The number of input channels\n",
    "                c_out: The number of output channels\n",
    "                k_size: The size of the convolution filter\n",
    "                k_pad: The amount of padding around the images\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, kernel_size = k_size, padding = k_pad, stride = 1),\n",
    "            nn.GroupNorm(8, c_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c_out, c_out, kernel_size = k_size, padding = k_pad, stride = 1),\n",
    "            nn.GroupNorm(8, c_out)\n",
    "        )\n",
    "        reinit_layer(self.block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "\n",
    "            Returns:\n",
    "                The output from the layer\n",
    "        \"\"\"\n",
    "        return self.block(x)\n",
    "\n",
    "class TransposeConvBlock(nn.Module):\n",
    "    \"\"\"A tranpose convolution block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, k_size = 3, k_pad = 1):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                c_in: The number of input channels\n",
    "                c_out: The number of output channels\n",
    "                k_size: The size of the convolution filter\n",
    "                k_pad: The amount of padding around the images\n",
    "        \"\"\"\n",
    "        super(TransposeConvBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_in, c_out, kernel_size = k_size, padding = k_pad, output_padding = 1, stride = 2),\n",
    "            nn.GroupNorm(8, c_out),\n",
    "            nn.ReLU(inplace=True))\n",
    "        reinit_layer(self.block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "\n",
    "            Returns:\n",
    "                The output from the layer\n",
    "        \"\"\"\n",
    "        return self.block(x)\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    \"\"\"A sigmoid activation function that supports categorical cross-entropy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_range = None):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                out_range: A tuple covering the minimum and maximum values to map to\n",
    "        \"\"\"\n",
    "        super(Sigmoid, self).__init__()\n",
    "        if out_range is not None:\n",
    "            self.low, self.high = out_range\n",
    "            self.range = self.high - self.low\n",
    "        else:\n",
    "            self.low = None\n",
    "            self.high = None\n",
    "            self.range = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the sigmoid function.\n",
    "\n",
    "            Rescales to the specified range if provided during construction\n",
    "\n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "\n",
    "            Returns:\n",
    "                The (potentially scaled) sigmoid of the input\n",
    "        \"\"\"\n",
    "        if self.low is not None:\n",
    "            return torch.sigmoid(x) * (self.range) + self.low\n",
    "        else:\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "class ListModule(nn.Module):\n",
    "    \"\"\"A container for a list of modules.\n",
    "\n",
    "        This class provides flexibility for the network architecture by ensuring layers in a configurable\n",
    "        architecture are correctly registered with torch.nn.Module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                args: A list of modules to be added to the network\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        for i, module in enumerate(args):\n",
    "            self.add_module(str(i), module)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a module.\n",
    "\n",
    "            Args:\n",
    "                idx: The index of the module to be retrieved\n",
    "\n",
    "            Returns:\n",
    "                The requested module\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Retrieve an iterator for the modules\n",
    "\n",
    "            Returns:\n",
    "                An iterator for the modules\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retrieve the number of modules\n",
    "\n",
    "            Returns:\n",
    "                The number of modules\n",
    "        \"\"\"\n",
    "        return len(self._modules)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"A U-Net for semantic segmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, n_classes, depth = 4, n_filters = 16, drop_prob = 0.1, y_range = None):\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "            Args:\n",
    "                in_dim: The number of input channels\n",
    "                n_classes: The number of classes\n",
    "                depth: The number of convolution blocks in the downsampling and upsampling arms of the U (default: 4)\n",
    "                n_filters: The number of filters in the first layer (doubles for each downsample) (default: 16)\n",
    "                drop_prob: The dropout probability for each layer (default: 0.1)\n",
    "                y_range: The range of values (low, high) to map to in the output (default: None)\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        # Contracting Path\n",
    "        ds_convs = []\n",
    "        for i in range(depth):\n",
    "            if i == 0: ds_convs.append(ConvBlock(in_dim, n_filters * 2**i))\n",
    "            else: ds_convs.append(ConvBlock(n_filters * 2**(i - 1), n_filters * 2**i))\n",
    "        self.ds_convs = ListModule(*ds_convs)\n",
    "\n",
    "        ds_maxpools = []\n",
    "        for i in range(depth):\n",
    "            ds_maxpools.append(maxpool())\n",
    "        self.ds_maxpools = ListModule(*ds_maxpools)\n",
    "\n",
    "        ds_dropouts = []\n",
    "        for i in range(depth):\n",
    "            ds_dropouts.append(dropout(drop_prob))\n",
    "        self.ds_dropouts = ListModule(*ds_dropouts)\n",
    "\n",
    "        self.bridge = ConvBlock(n_filters * 2**(depth - 1), n_filters * 2**depth)\n",
    "\n",
    "        # Expansive Path\n",
    "        us_tconvs = []\n",
    "        for i in range(depth, 0, -1):\n",
    "            us_tconvs.append(TransposeConvBlock(n_filters * 2**i, n_filters * 2**(i - 1)))\n",
    "        self.us_tconvs = ListModule(*us_tconvs)\n",
    "\n",
    "        us_convs = []\n",
    "        for i in range(depth, 0, -1):\n",
    "            us_convs.append(ConvBlock(n_filters * 2**i, n_filters * 2**(i - 1)))\n",
    "        self.us_convs = ListModule(*us_convs)\n",
    "\n",
    "        us_dropouts = []\n",
    "        for i in range(depth):\n",
    "            us_dropouts.append(dropout(drop_prob))\n",
    "        self.us_dropouts = ListModule(*us_dropouts)\n",
    "\n",
    "        self.output = nn.Sequential(nn.Conv2d(n_filters * 1, n_classes, 1), Sigmoid(y_range))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "            Args:\n",
    "                x: The input to the layer\n",
    "\n",
    "            Returns:\n",
    "                The output from the layer\n",
    "        \"\"\"\n",
    "        res = x\n",
    "        conv_stack = []\n",
    "\n",
    "        # Downsample\n",
    "        for i in range(len(self.ds_convs)):\n",
    "            res = self.ds_convs[i](res); conv_stack.append(res)\n",
    "            res = self.ds_maxpools[i](res)\n",
    "            res = self.ds_dropouts[i](res)\n",
    "\n",
    "        # Bridge\n",
    "        res = self.bridge(res)\n",
    "\n",
    "        # Upsample\n",
    "        for i in range(len(self.us_convs)):\n",
    "            res = self.us_tconvs[i](res)\n",
    "            res = torch.cat([res, conv_stack.pop()], dim=1)\n",
    "            res = self.us_dropouts[i](res)\n",
    "            res = self.us_convs[i](res)\n",
    "\n",
    "        output = self.output(res)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126afb16-359d-49d4-b84e-e9b563ff0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.p\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set the various seeds and flags to ensure deterministic performance\n",
    "\n",
    "        Args:\n",
    "            seed: The random seed\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic = True   # Note, can impede performance\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def get_class_weights(stats):\n",
    "    \"\"\"Get the weights for each class\n",
    "\n",
    "        Each class has a weight inversely proportional to the number of instances in the training set\n",
    "\n",
    "        Args:\n",
    "            stats: The number of instances of each class\n",
    "\n",
    "        Returns:\n",
    "            The weights for each class\n",
    "    \"\"\"\n",
    "    weights = 1. / stats\n",
    "    return [weight / sum(weights) for weight in weights]\n",
    "\n",
    "\n",
    "def load_model(filename, num_classes, weights, device):\n",
    "    \"\"\"Load a model\n",
    "\n",
    "        Args:\n",
    "            filename: The name of the file with the pretrained model parameters\n",
    "            num_classes: The number of classes available to predict\n",
    "            weights: The weights to apply to the classes\n",
    "            device: The device on which to run\n",
    "\n",
    "        Returns:\n",
    "            A tuple composed (in order) of the model, loss function, and optimiser\n",
    "    \"\"\"\n",
    "    model = UNet(1, n_classes = num_classes, depth = 4, n_filters = 16, y_range = (0, num_classes))\n",
    "    model.load_state_dict(torch.load(filename, map_location=\"cpu\"))\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(torch.as_tensor(weights, device=device, dtype=torch.float))\n",
    "    optim = opt.Adam(model.parameters())\n",
    "    return model, loss_fn, optim\n",
    "\n",
    "\n",
    "def save_model(model, input, filename):\n",
    "    \"\"\"Save the model\n",
    "\n",
    "        The model is saved as both a pkl file and a TorchScript pt file, which can be loaded via\n",
    "            model.load_state_dict(torch.load(PATH))\n",
    "            model.eval()\n",
    "\n",
    "        Args:\n",
    "            model: The model to save\n",
    "            input: An example input to the model\n",
    "            filename: The output filename, without file extension\n",
    "    \"\"\"\n",
    "    eval_model = model.eval()\n",
    "    torch.save(eval_model.state_dict(), f\"{filename}.pkl\")\n",
    "    torch_script_model = torch.jit.trace(eval_model, input, check_trace=False)\n",
    "    torch_script_model.save(f\"{filename}_traced.pt\")\n",
    "\n",
    "\n",
    "def accuracy(pred, truth, type=None):\n",
    "    \"\"\"Get the network accuracy\n",
    "\n",
    "        Args:\n",
    "            pred: The network prediction\n",
    "            truth: The true class\n",
    "            type: The class whose accuracy should be determined (default: None - overall accuracy)\n",
    "\n",
    "        Returns:\n",
    "            The accuracy\n",
    "    \"\"\"\n",
    "    target = truth.squeeze(1)\n",
    "    mask = target != 0 if type is None else target == type\n",
    "    return (pred.argmax(dim=1)[mask] == target[mask]).float().mean()\n",
    "\n",
    "\n",
    "def create_model(num_classes, weights, device):\n",
    "    \"\"\"Create the model\n",
    "\n",
    "        Args:\n",
    "            num_classes: The number of classes available to predict\n",
    "            weights: The weights to apply to the classes\n",
    "            device: The device on which to run\n",
    "\n",
    "        Returns:\n",
    "            A tuple composed (in order) of the model, loss function, and optimiser\n",
    "    \"\"\"\n",
    "    model = UNet(1, n_classes = num_classes, depth = 4, n_filters = 16, y_range = (0, num_classes))\n",
    "    loss_fn = nn.CrossEntropyLoss(torch.as_tensor(weights, device=device, dtype=torch.float))\n",
    "    optim = opt.Adam(model.parameters())\n",
    "    return model, loss_fn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92beaead-5325-447b-97e9-a25aab14b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# This line is important for GPU running, otherwise some weights end up on the CPU\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "view = \"C\"\n",
    "the_seed = 42\n",
    "gpu = torch.device('cuda:0')\n",
    "batch_size = 32\n",
    "NUM_CLASSES = 5   # Standard case with SHOWER = 1, TRACK = 2, DIFFUSE = 3, NUll = 0\n",
    "# Standard case with MIP = 1, HIP = 2, SHOWER = 3, DIFFUSE = 4, NUll = 0\n",
    "\n",
    "set_seed(the_seed)\n",
    "bunch = SegmentationBunch(f\"Images{view}\", \"Hits\", \"Truth\", batch_size=batch_size, device=gpu)\n",
    "train_stats = bunch.count_classes(NUM_CLASSES)\n",
    "weights = get_class_weights(train_stats)\n",
    "print(weights)\n",
    "\n",
    "model, loss_fn, optim = create_model(NUM_CLASSES, weights, gpu)\n",
    "\n",
    "n_epochs = 20\n",
    "train_losses = torch.zeros(n_epochs * len(bunch.train_dl), device=gpu)\n",
    "val_losses = torch.zeros(n_epochs, device=gpu)\n",
    "batch_losses = torch.zeros(len(bunch.valid_dl), device=gpu)\n",
    "\n",
    "train_accs = torch.zeros([NUM_CLASSES, n_epochs * len(bunch.train_dl)], device=gpu)\n",
    "val_accs = torch.zeros([NUM_CLASSES, n_epochs], device=gpu)\n",
    "batch_accs = torch.zeros([NUM_CLASSES, len(bunch.valid_dl)], device=gpu)\n",
    "\n",
    "import time\n",
    "t0 = time.perf_counter()\n",
    "i = 0\n",
    "set_seed(the_seed)\n",
    "for e in tqdm(range(0, n_epochs), desc=\"Training\"):\n",
    "    model = model.train()\n",
    "    n_batches = len(bunch.train_dl)\n",
    "    for b, batch in enumerate(bunch.train_dl):\n",
    "        x, y = batch\n",
    "        pred = model.forward(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        train_losses[i] = loss.item()\n",
    "        train_accs[0][i] = accuracy(pred, y)\n",
    "        train_accs[1][i] = accuracy(pred, y, type = 1)\n",
    "        train_accs[2][i] = accuracy(pred, y, type = 2)\n",
    "        train_accs[3][i] = accuracy(pred, y, type = 3)\n",
    "        train_accs[4][i] = accuracy(pred, y, type = 4)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #scheduler.step()\n",
    "        optim.zero_grad()\n",
    "        i += 1\n",
    "        if b == (n_batches - 1):\n",
    "            save_model(model, x, f\"unet_{view}_{e}\")\n",
    "    start = e * batch_size\n",
    "    finish = start + batch_size\n",
    "    print(f\"Train - Loss: {train_losses[start:finish].mean():.3f} MIP: {train_accs[1][start:finish].mean():.3f} HIP: {train_accs[2][start:finish].mean():.3f} Shower: {train_accs[3][start:finish].mean():.3f}  Diffuse: {train_accs[4][start:finish].mean():.3f}\")\n",
    "    \n",
    "    # Validate\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for b, batch in enumerate(bunch.valid_dl):\n",
    "            x, y = batch\n",
    "            pred = model.forward(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            batch_losses[b] = loss.item()\n",
    "            batch_accs[0][b] = accuracy(pred, y)\n",
    "            batch_accs[1][b] = accuracy(pred, y, type = 1)\n",
    "            batch_accs[2][b] = accuracy(pred, y, type = 2)\n",
    "            batch_accs[3][b] = accuracy(pred, y, type = 3)\n",
    "            batch_accs[4][b] = accuracy(pred, y, type = 4)\n",
    "        val_losses[e] = torch.mean(batch_losses)\n",
    "        val_accs[0][e] = torch.mean(batch_accs[0][~torch.isnan(batch_accs[0])])\n",
    "        val_accs[1][e] = torch.mean(batch_accs[1][~torch.isnan(batch_accs[1])])\n",
    "        val_accs[2][e] = torch.mean(batch_accs[2][~torch.isnan(batch_accs[2])])\n",
    "        val_accs[3][e] = torch.mean(batch_accs[3][~torch.isnan(batch_accs[3])])\n",
    "        val_accs[4][e] = torch.mean(batch_accs[3][~torch.isnan(batch_accs[4])])\n",
    "    print(f\"Valid - Loss: {val_losses[e]:.3f} MIP: {val_accs[1][e]:.3f} HIP: {val_accs[2][e]:.3f} Shower: {val_accs[3][e]:.3f}  Diffuse: {val_accs[4][e]:.3f}\")\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Networked trained in {t1 - t0:0.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167dd0f-1848-48a0-bbcb-f7957cab0918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584213bd-7ff9-4f05-a685-facb201c7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "binning = np.linspace(0, NUM_CLASSES, NUM_CLASSES + 1, dtype=int)\n",
    "\n",
    "model = model.to(gpu)\n",
    "confusion = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "for img, cls in bunch.valid_dl:\n",
    "    img = img.to(gpu)\n",
    "    output = model(img)\n",
    "    _, preds = torch.max(output, 1)\n",
    "    \n",
    "    cls_detached = cls.cpu().numpy().flatten()\n",
    "    preds_detached = preds.cpu().numpy().flatten()\n",
    "    \n",
    "    H, *_ = stats.binned_statistic_2d(preds_detached, cls_detached, None,\n",
    "                                      bins=[binning, binning], statistic='count')\n",
    "    confusion += H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df049f-503b-4f00-b4c4-48ddad60b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e06384-81a7-42ed-b015-48dce0b3234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e11 = np.sum(confusion[1:3,1:3])\n",
    "e12 = np.sum(confusion[1:3,3:])\n",
    "e21 = np.sum(confusion[3:,1:3])\n",
    "e22 = np.sum(confusion[3:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562c2df-a4e8-4c6d-84cb-f3a1fe6ec782",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_confusion = np.vstack((np.hstack((e11,e12)), np.hstack((e21, e22))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe79e8-285c-413c-a15d-8a5e7c643f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(reduced_confusion, axis=1).repeat(2).reshape((2,2))\n",
    "reduced_confusion /= sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cec9fa-8af0-4b0e-ab0d-a0554c8dda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec7a8e2-b127-4414-9d50-940ec72cf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = confusion.copy()\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plt.xlabel('true class')\n",
    "plt.ylabel('fraction')\n",
    "plt.step(list(np.arange(1, NUM_CLASSES)), np.sum(temporary[1:], axis=1) / np.sum(temporary[1:]), where=\"mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4a3a7-d2e8-4282-b8e4-e0d101537d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(confusion, axis=1).repeat(NUM_CLASSES).reshape((NUM_CLASSES,NUM_CLASSES))\n",
    "confusion /= sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5a345-5d4f-4e01-a8e2-d9a2b09af97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Class Accuracy\")\n",
    "for t in range(confusion.shape[0]):\n",
    "    print(f\"{t:2}: {100*(confusion[t,t] / confusion[t].sum()):.1f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6b5c5-d048-4653-bbd7-8c3f3d030b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion[0,:] = 0\n",
    "confusion[:,0] = 0\n",
    "\n",
    "sums = np.sum(confusion, axis=1).repeat(NUM_CLASSES).reshape((NUM_CLASSES,NUM_CLASSES))\n",
    "sums[0,:] = 1\n",
    "confusion /= sums\n",
    "\n",
    "print(f\"--- Class Accuracy\")\n",
    "for t in range(confusion.shape[0]):\n",
    "    print(f\"{t:2}: {100*confusion[t,t]:.1f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d777fd5-5cd0-4c31-ab2c-b2f223af4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(confusion[1:,1:3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d5ae6-d0a8-45f1-b411-7a58830848a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.xlabel('truth')\n",
    "plt.ylabel('network')\n",
    "plt.imshow(confusion)\n",
    "plt.colorbar()\n",
    "\n",
    "for t in range(1, NUM_CLASSES):\n",
    "    for n in range(1, NUM_CLASSES):\n",
    "        print(f\"{confusion[t, n]:.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f89ad-6749-419d-a0b8-a52a116cb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(2):\n",
    "    for n in range(2):\n",
    "        print(f\"{reduced_confusion[t, n]:.2f}\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cacdd10-bbc7-4634-83d4-8a49d0330b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
